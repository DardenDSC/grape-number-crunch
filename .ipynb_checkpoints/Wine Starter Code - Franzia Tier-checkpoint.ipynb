{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Grape Number Crunch!\n",
    "## Franzia Tier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/catalina_wine_mixer.gif\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Contest Overview](#Contest-Overview)\n",
    "* [Classification Overview](#Classification-Overview)\n",
    "* [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "* [Classification Metrics](#Classification-Metrics)\n",
    "* [Classification Techniques](#Classification-Techniques)\n",
    "  * [Logistic Regression](#Logistic-Regression)\n",
    "  * [Penalized Logistic Regression](#Penalized-Logistic-Regression)\n",
    "  * [Linear Discriminant Analysis](#Linear-Discriminant-Analysis)\n",
    "* [Recap](#Recap)\n",
    "* [Next Steps](#Next-Steps)\n",
    "* [Make Predictions](#Make-Predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contest Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are very excited to host the very first Grape Number Crunch with WACC! We think this contest will be a lot of fun and is (probably) the nerdiest possible way to enjoy wine.\n",
    "\n",
    "The premise of the contest is simple, you can team up with up to 3 other Darden students to build a model that predicts whether a given wine is a white or a red:\n",
    "\n",
    "<img src = \"images/this_is_a_white.gif\">\n",
    "\n",
    "**The top 4 FYs and top 4 SYs with the best models will win gift cards to one of Charlottesville's top vineyards, Pippin Hill**.\n",
    "\n",
    "<img src = \"images/trippin_hill.jpg\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification in machine learning is simply trying to predict a class label given some input data. Common examples include predicting whether or not a customer will churn next period, predicting if an email is spam or ham, or, in our case, if a wine is white or red. You could also have multiple classes, like where you can try to use image data of handwritten digits to determine which digit from 0-9 someone wrote (so 10 classes in total), or if you wanted to classify red wines vs. white wines vs. ros√©s (3 classes).\n",
    "\n",
    "Below we'll walk through some python code that will allow you to build your very own classification model! \n",
    "\n",
    "If you need some refreshers on coding in python, the Data Science Club highly recommends [DataCamp](datacamp.com) (free to all DSC members) or Coursera for some of their python tutorials. The DSC has also recently published some detailed descriptions in the [Bachelor Forecasting Challenge](https://github.com/DardenDSC/bachelor-forecasting-challenge/blob/master/Bachelor%20Starter%20Code.ipynb) that aims to provide some more background on ML, python, and general stats knowledge.\n",
    "\n",
    "Without further ado, let's channel our inner sommelier and start classifying wines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Build data visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "#For model training\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#For model assessment\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, auc, roc_curve, plot_roc_curve\n",
    "\n",
    "# Set number of CPU cores for parallel algorithms (code written by Prof. Albert)\n",
    "import os\n",
    "if \"CPU_LIMIT\" in os.environ:\n",
    "    # If you are on JupyterHub, this gives you the right number of CPUs for your virtual machine\n",
    "    num_cpus = int(os.getenv(\"CPU_LIMIT\").split('.')[0])\n",
    "else:\n",
    "    # If you are not on JupyterHub, this gives you the right number for your computer.\n",
    "    num_cpus = os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in our data from a csv file\n",
    "df = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the first 10 rows in our dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we have 13 columns in our dataset. The first 12 are numeric and the last is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a little over 3000 different wines in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a variable for our classification problem, where we say that white wines have a value of 1 and red wines have a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['white?'] = (df['color'] == 'white').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a summary of our data using a function that will give us a bunch of summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function \n",
    "#Function originally written by Prof. Michael Albert\n",
    "def summarize_dataframe(df):\n",
    "    \"\"\"Summarize a dataframe, and report missing values.\"\"\"\n",
    "    missing_values = pd.DataFrame({'Variable Name': df.columns,\n",
    "                                   'Data Type': df.dtypes,\n",
    "                                   'Missing Values': df.isnull().sum(),\n",
    "                                   'Unique Values': [df[name].nunique() for name in df.columns]}\n",
    "                                 ).set_index('Variable Name')\n",
    "    with pd.option_context(\"display.max_rows\", 1000):\n",
    "        display(pd.concat([missing_values, df.describe(include='all').transpose()], axis=1).fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some highlights:\n",
    " * About 75% of the training set is of white wines, with the remaining being reds\n",
    " * Residual sugar has a lot of variation, and it's mean appears far different than it's median, indicating skew in the data\n",
    " * Median quality of the wines is a 6, while the max is a 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to visualize our data. We'll start by looking at the distributions of each column in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in range(0,4):\n",
    "    for j in range(0,3):\n",
    "        col = df.columns[k]\n",
    "        axs[i,j].hist(df[col], bins = 25)\n",
    "        axs[i,j].set_title(col)\n",
    "        k = k+1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can break these histograms down further by looking at the distributions of red and white wines separately for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_colors = ['#dbdd46','#5F021F']\n",
    "sns.set_palette(sns.color_palette(wine_colors))\n",
    "\n",
    "fig, axs = plt.subplots(4, 3)\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in range(0,4):\n",
    "    for j in range(0,3):\n",
    "        col = df.columns[k]\n",
    "        sns.kdeplot(df[col], hue = df['color'], shade = True, ax = axs[i,j])\n",
    "        k = k+1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like many of these columns follow normal or [exponential distributions](https://en.wikipedia.org/wiki/Exponential_distribution).\n",
    "\n",
    "The distributions of residual sugar and free sulfur dioxide between red and white wines appear to be very different. These could be a good features to include in our classification model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the relationships between variables with a correlation plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.triu(np.ones_like(corr, dtype=bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin = -1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the bottom row, we can see which variables are storngly correlated with white wines vs. reds.\n",
    "\n",
    "* Positive correlation with white (have a red-shaded cell):\n",
    "  * Citric acid\n",
    "  * Residual sugar\n",
    "  * Sulfur dioxide\n",
    "* Negative correlation with white (have a blue-shaded cell):\n",
    "  * Acidity\n",
    "  * Chlorides\n",
    "  * Density\n",
    "  * pH\n",
    "  * Sulfates\n",
    "  \n",
    "There are also some strong relationships between predictor variables, indicated by the magnitude of the correlation coefficient:\n",
    " * Alcohol and quality (go figure)\n",
    " * Alcohol and density\n",
    " * Density and residual sugar\n",
    " * Density and fixed acidity\n",
    " * Free and total sulfur dioxide\n",
    " * Volatile acidity and citric acid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the relationships and distrubtions between multiple variables using a 2D kernel density plot! Each axis represents a distribution of a variable, and the contours represent the shape of a distribution. You can think of these like the contour maps you see plotting elevation vs. longitude and latitude. The peaks of the mountain tops on a map like that are analogous to the peak (or mode) of a distribution of data.\n",
    "\n",
    "<img src = 'images/contour_plot.png'>\n",
    "\n",
    "Basically, we are comparing two distributions of data (white wines and red wines) on 2 axes instead of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x = df['alcohol'], y= df['density'], hue = df['color'], color =  wine_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red wines have slightly higher density for the given level of alcohol than white wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x = df['density'], y= df['residual sugar'], hue = df['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White wines have much more variability with residual sugars than reds, and these tend to lead to higher density levels in white wines while having very little effect on red wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x = df['density'], y= df['fixed acidity'], hue = df['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red wines have much more variability in fixed acidity, while white wines have more variability in density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x = df['free sulfur dioxide'], y= df['total sulfur dioxide'], hue = df['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free sulfur dioxide and total sulfur dioxide are highly correlated in both kinds of wine, and white wines typically have higher sulfur dioxide levels than red wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(x = df['volatile acidity'], y= df['citric acid'], hue = df['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red wines tend to have higher volatile acidity than white wines, but both seem to have similar levels of citric acid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split our dataset in to a training set and a validation set. In this case, we are using 80% of the rows of our training data to fit our model and using the remaining 20% to validation our results. \n",
    "\n",
    "You might be wondering why we dont use all of our training data to fit our model. The reason is that we want to avoid **overfitting** our model to our training data. When we overfit our model to our training data, our model tends to perform poorly on new, unseen data because our model will effectively be memorizing our training data.\n",
    "\n",
    "There's an inherent tradeoff in splitting our data between our training set and our validation set. By giving our model more data in the training set, we will be able to come up with better model parameters for our model. But leaving more data for the validation set will allow us to more accurately estimate the model's performance on unseen data. Hence there's a natural tradeoff between estimating model parameters (e.g. regression coefficients) and estimating our model's performance (e.g. model accuracy on unseen data). We want to find the right balance between estimating model parameters and estimating model performance.\n",
    "\n",
    "A typical rule of thumb is to reserve 20% of your data for your validation set. Depending on your data, you might find that you need more rows in your validation set (especially if your overall dataset doesn't have many rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data was already shuffled before it was loaded, but typically you want to have shuffle = True\n",
    "#I used shuffle = False here to make merging datasets easier later in the code\n",
    "\n",
    "train_df, validation_df = train_test_split(df, test_size = 0.2, random_state = 37, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the features we want to use in our model\n",
    "features = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "       'pH', 'sulphates', 'alcohol', 'quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR: Split your training data into a training set and validation set to accurately estimate model parameters (training) as well as model performance (validation).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize our Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to **standardize** our data across different columns. This will allow our different features to have a similar range of values, which will help us with some of the models we'll be working with.\n",
    "\n",
    "For example, notice how the range of values between `Fixed Acidity` and `Volatile Acidity` are very different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Range of values for Fixed Acidity: \" + str(train_df['fixed acidity'].max() - train_df['fixed acidity'].min()))\n",
    "print(\"Range of values for Volatile Acidity: \" + str(train_df['volatile acidity'].max() - train_df['volatile acidity'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the range of values for Fixed Acidity are an order of magnitude higher than Volatile Acidity. We want our different columns to have similar ranges of values so we can apply **regularization** techniques to these columns. We can do this by calculating the mean and the standard deviation of each column, and performing the following calculation on on our data. \n",
    "\n",
    "standardized value $ = \\frac{x - \\mu}{\\sigma} $\n",
    "\n",
    "where:\n",
    " - $x$ is a data point in a given column\n",
    " - $\\mu$ is the column's mean value\n",
    " - $\\sigma$ is the column's standard deviation\n",
    "\n",
    "This will ensure that the data between columns will have similar ranges of values.\n",
    "\n",
    "What is regularization and why does it matter to us? Regularization is simply a way of preventing overfitting in our model by penalizing our model for *unnecessary complexity*. You can think of regularization like [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor), where we want the simplest possible explanation to our problem but cutting out all the noise. \n",
    "\n",
    "Below we are going to scale all of our continuous data, as listed in `continuous_cols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "                   'total sulfur dioxide', 'density','pH', 'sulphates', 'alcohol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_continuous_cols = ['quality', 'color', 'white?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will calculating the column means and standard deviations for all the data in our training dataset.\n",
    "scaler.fit(train_df[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will scale the data in our training dataset (subtract the mean and divide by the standard deviation)\n",
    "scaled_continuous_train_df = pd.DataFrame(scaler.transform(train_df[continuous_cols]), columns = continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we merge our scaled data back with on non-continuous data into one dataframe using the merge function\n",
    "scaled_train_df = pd.merge(scaled_continuous_train_df, train_df[non_continuous_cols], left_index = True, right_index = True)\n",
    "scaled_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We repeat this process for our validation set\n",
    "scaled_continuous_validation_df = pd.DataFrame(scaler.transform(validation_df[continuous_cols]), columns = continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_continuous_validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we merge our scaled data back with on non-continuous data into one dataframe using the merge function\n",
    "scaled_validation_df = pd.merge(scaled_continuous_validation_df, validation_df[non_continuous_cols].reset_index(), left_index = True, right_index = True)\n",
    "scaled_validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've scaled our training and validation data, we can see that the ranges of our columns are more or less equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Range of values for Fixed Acidity: \" + str(scaled_train_df['fixed acidity'].max() - scaled_train_df['fixed acidity'].min()))\n",
    "print(\"Range of values for Volatile Acidity: \" + str(scaled_train_df['volatile acidity'].max() - scaled_train_df['volatile acidity'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now split our scaled data into our features / input variables (denoted by X) and our response / output variable (denoted by y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaled_train_df[features]\n",
    "y_train = scaled_train_df['white?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = scaled_validation_df[features]\n",
    "y_validation = scaled_validation_df['white?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR: Standardizing your data can help with some regularization techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data transformed into a format to start building models, we need to start thinking about how we should assess the performance of our models. \n",
    "\n",
    "With classification problems like this, there are a number of metrics we can use to assess model performance. We'll start with some classic metrics that come from the aptly named **Confusion Matrix**:\n",
    "\n",
    "<img src = 'images/confusionMatrix.png'>\n",
    "\n",
    "Image Source: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n",
    "\n",
    "The confusion matrix is a 2x2 framework (MBB consultants please contain your excitement) for assessing the types of errors that occur in classification problems. On the x-axis you have the actual values of the thing you are trying to classify (is the wine red or white in reality?) and on the y-axis you have your model's predictions (is the wine red or white according to our model's predictions?). You also have the notion of *positive* and *negative* which depends on how your dataset is defined. In our example, a positive (or a value of 1) means that the wine is white, and a negative means the wine is red. It's important to make sure you understand what a positive value versus a negative value is in your data when doing these kinds of classification problems so you can correctly interpret the results!\n",
    "\n",
    "Next we have **true positives** and **false positives**. A true positive (AKA **TP**) is when your model predicts the wine as being white (positive) and the wine in reality is actually white. A false positive (AKA **FP**) is when your model predicts the wine as being white (positive) when in reality the wine is red (negative). \n",
    "\n",
    "Conversely, we have **true negatives** and **false negatives**. A true negative (AKA **TN**) is when your model predicts the wine as a red (negative) and the wine is actually red in reality (negative). A false negative (AKA **FN**) is when your model predicts the wine as red (negative) when in reality the wine is white (positive).\n",
    "\n",
    "This is intuitive image that shows the differences between TPs, FPs, TNs, and FNs, where a positive value indicates the person is pregnant:\n",
    "\n",
    "<img src = 'images/confusionMatrix2.png'>\n",
    "\n",
    "Image source: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these definitions, we can start to define some metrics to assess model accuracy. 3 of the most common metrics that come from the confusion matrix are **accuracy**, **precision**, and **recall**.\n",
    "\n",
    "Accuracy is simply measure of the proportion of objects you classified correctly (TPs and TNs) over all objects in your dataset. In other words, accuracy means \"what percentage of objects did your model classify correctly?\"\n",
    "\n",
    "Precision measures the proportion of objects your model correctly classified as positive (TPs) over the total number of objects your model classified as positive (TPs and FPs). In other words, precision means \"what percentage of your model's predictions that are positive are actually positive?\" Precision is a great metric to use when the cost of making a false positive is relatively high, like with email spam classification. Assuming that a positive classification indicates that the email is spam, a false positive would mean that our spam detector would think that the email is spam when in reality it is \"ham\", or something we would want to read. Thus, having high precision would be very important to us in this kind of classification problem.\n",
    "\n",
    "Recall measures the proporition of objects your model correctly classifies as positive (TPs) over the total number of objects that are actually positive (TPs and FNs). In other words, recall means \"what proportion of positive values (i.e. white wines) did my model correctly classify?\" Recall is a great metric when the cost of a false negative is really high. For example, think of medical diagnosis tests like detecting cancer in patients. We would want a model with high recall, one that catches as many patients with cancer as possible so they could get treatment. A false negative would mean that person would be left untreated, so we would want to minimize that value as much as possible.\n",
    "\n",
    "Here's another good visualization depicting the differences between precision and recall:\n",
    "\n",
    "<img src = 'images/precision_recall.png'>\n",
    "\n",
    "Image source: https://towardsdatascience.com/whats-the-deal-with-accuracy-precision-recall-and-f1-f5d8b4db1021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start using these three metrics in our upcoming models!\n",
    "\n",
    "We'll start by generating a baseline to compare our future models against. Since about 75% of our training data is white wine and the rest of reds, a naive classifier would simply always predict that a wine is white since this would guarantee about 75% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_class_forecast = np.repeat(1, validation_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true = y_validation, y_pred = naive_class_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_true = y_validation, y_pred = naive_class_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_true = y_validation, y_pred = naive_class_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is a combination of precision and recall according to the following formula:\n",
    "\n",
    "$ F_1 = 2 \\frac{precision \\times recall}{precision + recall}$\n",
    "\n",
    "F1 is a harmonic mean of precision and recall, which will balance the two scores together. F1 is often favored over accuracy when the costs of False Positives and False Negatives are both very high and/or when there is an imbalance in the classes we are trying to classify (e.g. our training set is mostly white wines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_true = y_validation, y_pred = naive_class_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about one more metric for classification methods: **log-loss**. To understand this metric, we should now switch our thinking about what exactly our models will be predicting. Before, with the confusion matrix paradigm, our models were making **hard classifications**: predicting a given wine is 100% a white or this wine is 100% a red. Our models can also make predictions on the *probability* that a wine is white. More certain classifications that a wine is white [or red] will be very close to 1 [or 0], while uncertain classificaitons that a wine is white [or red] will be close to 0.5. \n",
    "\n",
    "Log-loss is a metric that's very useful in classification problems when we are predicting probabilities rather than hard classifications. While the background on log-loss is more complicated and outside the scope of this notebook, *our goal is to minimize a model's log-loss*.\n",
    "\n",
    "#### Math (feel free to skip)\n",
    "\n",
    "$log loss = -\\frac{1}{N} \\sum_{i=1}^{N} y_i log(\\hat{y_i}) + (1- y_i) log(1 - \\hat{y_i} ) $\n",
    "\n",
    "where:\n",
    " - $y_i$ is the actual value (1 = white, 0 = red)\n",
    " - $\\hat{y_i}$ is the predicted probabilty from your model (between 0 and 1)\n",
    " \n",
    "Note that when your model makes a confident prediction that is true (model predicts 0.99 white and the wine is white), that log-loss is very small for that prediction:\n",
    "\n",
    "$ -[\\boldsymbol{1 * log(0.99)} + (1 - 1) log(1 - 0.99)]= -[-0.01 + 0] = \\boldsymbol{0.01} $\n",
    "\n",
    "But when your model makes a confience prediction that is wrong (model predicts 0.99 white and the wine is red), then the log-loss is massive!\n",
    "\n",
    "$ -[0 * log(0.99) + \\boldsymbol{(1 - 0) log(1 - 0.99)}] = -[0 + -4.61] = \\boldsymbol{4.61} $\n",
    "\n",
    "**Note**: I've bolded the parts of the equation where the values don't cancel to zero to highlight the impact that a predicted probability has on the log-loss calculation.\n",
    "\n",
    "#### End of optional math\n",
    "\n",
    "A naive forecast in this case will simply predict the prior probability of a wine being white based upon the training set. So in this case, our naive forecast will predict a probability of being a white wine consistently at 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_prob_forecast = np.repeat(np.mean(df['white?']), validation_df.shape[0])\n",
    "naive_prob_forecast[:10] #show the first 10 predictions in our naive forecast, all being 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_true = y_validation, y_pred = naive_prob_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR: Pick the classification metric that best suits the kind of problem you're trying to solve**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kick things off, we will look at **logistic regression** as a classification method. To make sense of logistic regression, it helps to think back to linear regression and how we formulated that model [in our previous contest](https://github.com/DardenDSC/bachelor-forecasting-challenge/blob/master/Bachelor%20Starter%20Code.ipynb).\n",
    "\n",
    "Linear Regressions fall under a broader class of models known as Linear Models. These models take the form of a weighted sum of predictor variables:\n",
    "\n",
    "$ y = \\beta_0 + \\sum \\beta_j x_j + \\epsilon $\n",
    "\n",
    "where:\n",
    "- $ x $ is a row of our $j$ predictor variables (i.e. a single contestants occupation, age, if they got a first impression rose or not, etc)\n",
    "- $ y $ is the response variable (the week a contestant is eliminated)\n",
    "- $ \\beta_0 $ is the intercept of the model, or the default value of the response variable when all predictor variables equal zero\n",
    "- $ \\beta_j $ is a coefficient for a predictor variable, with j total predictor variables, and indicates the relationship between a predictor variable and the response variable\n",
    "- $ \\epsilon $ is the error term of our model, which we assume is normally-distributed with a mean of zero\n",
    "\n",
    "With our wine data, we could try to use some variables to predict alcohol content of a wine, *a continuous variable*, but providing some input variables like residual sugar, density, or wine color and this would be a good use of linear regression.\n",
    "\n",
    "However, we are interested in predicting the probability that a wine is white or red, which is bounded by 0 and 1. It doesn't really make sense to predict values greater than 1 or less than 0 when it comes to probabilities. To account for this, we need to make sure our predictions fall between a range of 0 and 1. To this, we can wrap our linear regression model with a sigmoid function, which will change the shape of our curve from a straight line to more of an S-shape:\n",
    "\n",
    "<img src = 'images/linear_vs_logistic.png'>\n",
    "\n",
    "Image source: https://www.javatpoint.com/linear-regression-vs-logistic-regression-in-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, we can represent a logistic regression model as such:\n",
    "\n",
    "$ P(Y = 1) = \\sigma(\\beta_0 + \\sum \\beta_j x_j) $\n",
    "\n",
    "where:\n",
    " - $\\sigma$ is the sigmoid function that make our line S-shaped - $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    " - $Y$ is now our predicted probability of an observation being a white wine (Y = 1)\n",
    " - and every other variable has the same interpretation as with a linear regression\n",
    " \n",
    "You could also rewrite this formulation as:\n",
    "\n",
    "$ P(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\sum \\beta_j x_j)}} $\n",
    "\n",
    "This new formula ensures that we will not predict a probability of greater than 1 or less than 0. There are other benefits to this method for predicting probabilities, but that's outside the scope of this notebook.\n",
    "\n",
    "**TL;DR: Logistic regression smushes our response variables between 0 and 1 and while still drawing upon ideas from good ol' linear regression**\n",
    "\n",
    "Now we can create a model that will output a predicted probability of a wine being white versus red!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by selecting features for our model. Feel free to add or remove features that you think might be helpful in classifying reds vs. whites!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feel free to add or remove any variables here if you think it will help the model classify between red and white wines!\n",
    "model_features = ['residual sugar','free sulfur dioxide']\n",
    "\n",
    "model_train = X_train[model_features]\n",
    "model_validation = X_validation[model_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our model here. max_iter means the number of iterations we need to update our model parameters (our Betas) to converge to a \"good enough\" model\n",
    "LogReg = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit our model to our data, providing our input variables `LogReg_train` and our output labels `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg.fit(model_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions with our validation set, which our model has not seen yet, to see how well it performs on new data.\n",
    "\n",
    "We can use the `predict` function to generate hard classifications for these new wines, so the outputs will be 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_pred_class = LogReg.predict(model_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 30 predictions from our validation set\n",
    "LogReg_pred_class[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate accuracy and f1 scores from these hard classifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model accuracy: \" + str(accuracy_score(y_true = y_validation , y_pred = LogReg_pred_class)))\n",
    "\n",
    "print(\"Model f1 score: \" + str(f1_score(y_true = y_validation , y_pred = LogReg_pred_class)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `predict_proba` function to generate probabilistic estimates for whether a given wine is a white or red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg_pred_prob = LogReg.predict_proba(model_validation)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 30 predictions from our validation set\n",
    "LogReg_pred_prob[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these predicted probabilities to assess our models log-loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model log-loss: \" + str(log_loss(y_true = y_validation, y_pred = LogReg_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So remember when we standardized our data so we could effectively use regularization? We've now come to the point in our analysis where this will pay off.\n",
    "\n",
    "Much like penalized linear regression, as seen in [our previous contest](https://github.com/DardenDSC/bachelor-forecasting-challenge/blob/master/Bachelor%20Starter%20Code.ipynb), penalized logistic regression will help us find the optimal complexity for our model by balancing the bias-variance tradeoff. \n",
    "\n",
    "**Note**: if you need a refresher on concepts like model complexity and the bias-variance tradeoff, be sure to read through that section in our [Bachelor Forecasting Challenge notebook](https://github.com/DardenDSC/bachelor-forecasting-challenge/blob/master/Bachelor%20Starter%20Code.ipynb). \n",
    "\n",
    "But if you're lazy and don't want to read through all of that, here's the TL;DR: \n",
    "\n",
    "**TL;DR: We're introducing a penalty term to automatically penalize our models for added complexity. This will actually help our models better generalize to unseen data and find the optimal level of complexity for our model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a new feature to our model, `alcohol`, and see how this effects our outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feel free to add or remove any variables here if you think it will help the model classify between red and white wines!\n",
    "model_features = ['residual sugar','free sulfur dioxide','alcohol']\n",
    "\n",
    "model_train = X_train[model_features]\n",
    "model_validation = X_validation[model_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different values of our penalty term to apply here in our model. Larger values for `C` in this case mean a stronger penalty that is being applied for complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geneate 10 different values for our penalty term to try in our model\n",
    "C = np.linspace(0.1, 1, 10)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use cross-validation to help us determine which value of `C` is best to use in our model. For a more detailed explanation of cross-validation and why it's useful, see our starter code on the Bachelor. Otherwise, the TL;DR on cross-validation is that it allows us to more accurately assess our model's performance on unseen data and can use more of your data in the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegPenalized = GridSearchCV(estimator = LogisticRegression(max_iter = 1000),\n",
    "                    param_grid = {'C':C},\n",
    "                    cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegPenalized.fit(model_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegPenalized.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make our hard classifications again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegPenalized_pred_class = LogRegPenalized.predict(model_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model accuracy: \" + str(accuracy_score(y_true = y_validation , y_pred = LogRegPenalized_pred_class)))\n",
    "\n",
    "print(\"Model f1 score: \" + str(f1_score(y_true = y_validation , y_pred = LogRegPenalized_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can generate predicted probabilities too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogRegPenalized_pred_prob = LogRegPenalized.predict_proba(model_validation)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model log-loss: \" + str(log_loss(y_true = y_validation, y_pred = LogRegPenalized_pred_prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the differences between our normal logistic regression and our penalized logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_metrics = {'Logistic Regression': \n",
    "                     {'accuracy':accuracy_score(y_true = y_validation , y_pred = LogReg_pred_class),\n",
    "                      'f1_score':f1_score(y_true = y_validation , y_pred = LogReg_pred_class),\n",
    "                      'log-loss':log_loss(y_true = y_validation, y_pred = LogReg_pred_prob)},\n",
    "                  'Penalized Logistic Regression':\n",
    "                     {'accuracy':accuracy_score(y_true = y_validation , y_pred = LogRegPenalized_pred_class),\n",
    "                      'f1_score':f1_score(y_true = y_validation , y_pred = LogRegPenalized_pred_class),\n",
    "                      'log-loss':log_loss(y_true = y_validation, y_pred = LogRegPenalized_pred_prob)}\n",
    "                 }\n",
    "\n",
    "pd.DataFrame(logreg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, our penalized logistic regression model does worse in terms of model accuracy and f1 score, but slightly better in terms of log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last up in the Franzia tier, we are covering **Linear Discriminant Analysis** (AKA \"LDA\"). This kind of analysis is useful when we have prior information on our labels. From our dataset, we can see that about 75% of wines we are classifying our white and only 25% red. If we believe that breakdown to remain the same in our future datasets (i.e. the person source our wine will continue to buy 3 bottles of white for every bottle of red), we can include this prior information in our analysis to better inform our classification methods. \n",
    "\n",
    "LDA comes from the field of Bayesian statistics, which is all about measuring degrees of belief that one has in a probablistic event occuring. The world of statistics often boils down into two schools of thought: **frequentist statistics** and **Bayesian statistics**. \n",
    "\n",
    "Frequentist stats comes from idea that we can infer the likelihood of some event occuring by collecting enough information in our sample data to make inferences about the probability of an event occuring in the future. Frequentists cannot include prior information in their analsyses, relying solely on the data they have collected to make inferences. If you took a stats class in high school of college, chances are that you worked primarily with frequentist statistics.\n",
    "\n",
    "Bayesian stats differs from frequentists in that this kind of analysis *allows for the inclusion of prior information*, such as the probabilty that a wine will be red or white before ever seeing the data. This can be highly subjective depending upon the problem and requires caution and good justification when making decisions about what prior information to include in an analysis. However, if good assumptions about the prior are made, our analyses can be much more powerful.\n",
    "\n",
    "This is a great [video](https://www.youtube.com/watch?v=7GgLSnQ48os) by Hannah Fry on Bayesian statistics that describes the idea an a very intuitive manner.\n",
    "\n",
    "**TL;DR: Bayesian stats allows for the incorporation of prior knowledge while frequentist stats does not**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional math section on Bayes Theorem\n",
    "\n",
    "The heart of Bayesian analysis comes from Bayes Theorem, as formulated below:\n",
    "\n",
    "$ P(\\theta | X ) = \\frac{P(X | \\theta) P(\\theta)}{P(X)} $\n",
    "\n",
    "where:\n",
    " - $\\theta$ represents the set of model parameters we are trying to estimate (in our example, the probability of a wine being white vs. red)\n",
    " - $X$ represents our data\n",
    " - $P(\\theta | X)$ is the **posterior probability**, or our updated belief about $\\theta$ after seeing the data\n",
    " - $P(X | \\theta)$ is the **likelihood** of seeing our data given some values of $\\theta$\n",
    " - $P(\\theta)$ is the **prior probability** of $\\theta$, or the prior information we have on our parameter of interest (the probability of wine being white or red)\n",
    " - $P(X)$ is the **evidence** or simply the probability of seeing the data\n",
    " \n",
    "The powerful idea behind Bayes Theorem is that we can continually update our belief about something (in this case, $\\theta$) by combining our prior knowledge (the prior probability) with observed data (shown in the likelihood and the evidence).\n",
    "\n",
    "#### End math stuff\n",
    "\n",
    "Coming back to LDA, LDA uses the **likelihood** and the **prior probabilities** to make classifications about our wines. You can think of likelihood as \"what are the chances that I would be seeing this kind of data if this wine were white [or red]?\" You can think of prior probabilities as \"based on my previous knowledge, what is the chance that any given wine would be a white [or red]?\"\n",
    "\n",
    "More specifically, LDA calculates ratios between the likelihoods and compares them to the ratios of the prior probabilities to make classifications:\n",
    "\n",
    "likelihood ratio $ = \\frac{P(X | wine = white)}{P(X | wine = red)} $\n",
    "\n",
    "prior ratio = $ \\frac{P(wine = red)}{P(wine = white)} $\n",
    "\n",
    "The way LDA makes classifications is by comparing the likelihood ratio to the prior ratio as shown above. If the likelihood ratio is greater than the prior ratio, we will guess the wine is white, if not, we'll guess the wine is red.\n",
    "\n",
    "Some assumptions that LDA makes about our data:\n",
    " - each feature in our dataset follows a normal distribution\n",
    " - each feature in our dataset has the same amount of variance and covariance with each other\n",
    "\n",
    "The reason that it's called *Linear* Discriminant Analysis is that this assumption about each variable having the same variance and coviarance with each other ultimately leads to linear decision boundaries in our classifier as opposed to non-linear decision boundaries. The image below shows the differences between LDA and what is known as QDA (Quadratic Discriminant Analysis). Typically, LDA performs better on problems than QDA since QDA has a tendency to overfit to its training data while LDA's restrictive linear decision boundary prevents overfitting.\n",
    "\n",
    "<img src = 'images/lda_vs_qda.png'>\n",
    "\n",
    "Image source: http://scikit-learn.sourceforge.net/0.6/auto_examples/plot_lda_vs_qda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try building a LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feel free to add or remove any variables here if you think it will help the model classify between red and white wines!\n",
    "model_features = ['residual sugar','free sulfur dioxide']\n",
    "\n",
    "model_train = X_train[model_features]\n",
    "model_validation = X_validation[model_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's include the prior information we have about our data in this model. Let's say that whoever is sourcing our wines for our dataset typically follows the rule that they will buy 3 white wines for every 1 bottle of red wine. We can encode this below as prior probabilities with a 0.25 chance of the wine being red and a 0.75 chance of the wine being white:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informed_prior = [0.25, 0.75]\n",
    "#uninformed_prior = [0.5, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that your choice of prior can be highly subjective, and that one should be sure to justify their decision for a prior before including it in one's analysis. Even if you don't have any prior information, one can still use an **uninformed prior** in Bayesian analysis, which is basically saying \"I have no freaking idea how prevalent white wines are versus red wines\".\n",
    "\n",
    "As you collect more data, the importance of your prior becomes less and less important. If your dataset is very small, then your choice of prior will have a much larger impact on your analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define and fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA = LinearDiscriminantAnalysis(priors = informed_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA.fit(model_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make hard classifications with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_pred_class = LDA.predict(model_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model accuracy: \" + str(accuracy_score(y_true = y_validation , y_pred = LDA_pred_class)))\n",
    "\n",
    "print(\"Model f1 score: \" + str(f1_score(y_true = y_validation , y_pred = LDA_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA_pred_prob = LDA.predict_proba(model_validation)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(y_true = y_validation, y_pred = LDA_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model log-loss: \" + str(log_loss(y_true = y_validation , y_pred = LDA_pred_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_metrics['LDA']= {'accuracy':accuracy_score(y_true = y_validation , y_pred = LDA_pred_class),\n",
    "                      'f1_score':f1_score(y_true = y_validation , y_pred = LDA_pred_class),\n",
    "                      'log-loss':log_loss(y_true = y_validation, y_pred = LDA_pred_prob)\n",
    "                 }\n",
    "\n",
    "pd.DataFrame(logreg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this table to compare the performance of these different models when trying to make your final predictions.\n",
    "\n",
    "**We will be assessing models according to log-loss, so try to optimize on that metric if you can!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "Logistic Regression:\n",
    " - Natural extention of linear regression in predicting probabilies\n",
    " - One of the most interpretable classification methods\n",
    " - Downside: no way to easily penalize for model complexity and adding too many features\n",
    " \n",
    "Penalized Logistic Regression:\n",
    " - Penalized unnecessary complexity with the penalty term\n",
    " - Helps perform feature selection automatically\n",
    " - Downside: takes longer to run since you need to find the optimal value for your penalty term\n",
    " \n",
    "LDA:\n",
    " - Can incoporate prior information with the use of informed priors and Bayesian analysis\n",
    " - Runs very quickly on large datasets\n",
    " - Downside: only works with numeric data, makes strong assumptions about the distributions of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that you've made some basic classification models, here are some ideas for next steps you can take in your modeling efforts:\n",
    "- Include and remove different variables to see how it impacts accuracy, f1 score, and log-loss\n",
    "- See if an uninformed prior works better in LDA than an informed prior\n",
    "- Experiment by creating new and potentially useful features from existing variables\n",
    "- Once you find your best model (and the optimal complexity parameters for that model) you can refit your model to both the training *and* validation data to get even better estimates of your model parameters. Only do this once you have done cross-validation on just your training data and you think you have good estimates of your complexity parameters.\n",
    "\n",
    "Once you are happy with your model, go ahead and generate predictions for your best-performing model below.\n",
    "\n",
    "**Models will be assessed by using the log-loss metric**. Thus, you want to build models that aim to **minimize** the model's log-loss.\n",
    "\n",
    "Once you're done with that and you've generated your predictions, save them to a .csv file with your team's name and upload them to the Data Science Club's MS Teams page. Further instructions will be on Teams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to apply the same transformations we made to our training data earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_continuous_test_df = pd.DataFrame(scaler.transform(test_df[continuous_cols]), columns = continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_df = pd.merge(scaled_continuous_test_df, test_df['quality'], left_index = True, right_index = True)\n",
    "scaled_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaled_test_df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select the same features we want to use in our final model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same features that you used in you earlier model! (under model features)\n",
    "model_features = ['residual sugar','free sulfur dioxide']\n",
    "\n",
    "model_test = X_test[model_features]\n",
    "\n",
    "LogRegTestPred = LogReg.predict_proba(model_test)[:,1]\n",
    "LogRegTestPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penalized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same features that you used in you earlier model! (under model features)\n",
    "model_features = ['residual sugar','free sulfur dioxide','alcohol']\n",
    "\n",
    "model_test = X_test[model_features]\n",
    "\n",
    "LogRegPenalizedTestPred = LogRegPenalized.predict_proba(model_test)[:,1]\n",
    "LogRegPenalizedTestPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Disciminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same features that you used in you earlier model! (under model features)\n",
    "model_features = ['residual sugar','free sulfur dioxide']\n",
    "\n",
    "model_test = X_test[model_features]\n",
    "\n",
    "LDATestPred = LDA.predict_proba(model_test)[:,1]\n",
    "LDATestPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME = \"_______\" #Enter your team name here\n",
    "\n",
    "#YOUR_NAME = \"BrianFosterPegg4Prez\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your final predictions where you see 'LogRegTestPred'\n",
    "FINAL_PREDS = pd.DataFrame(LogRegTestPred, columns = ['predicted_probability'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the number of predictions you've made matches the number of rows in the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of predictions: \" + str(len(FINAL_PREDS)))\n",
    "print(\"Number of rows in test data: \" + str(test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above two numbers match, you should be good to write your predictions to a csv file and submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_PREDS.to_csv(YOUR_NAME+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on building your first ML model!\n",
    "\n",
    "<img src = \"images/bfp_is_a_data_science.JPG\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
